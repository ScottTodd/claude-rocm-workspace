# PR Review: #3481 ‚Äî Run Linux Nightly Package Build Entirely Inside Container

* **PR:** [#3481](https://github.com/ROCm/TheRock/pull/3481)
* **Author:** chiranjeevipattigidi (chiranjeevi-amd)
* **Branch:** `users/cpattigi-amd/linux-nightly-container` ‚Üí `main`
* **Reviewed:** 2026-02-18
* **Files changed:** 2

---

## Summary

This PR migrates `release_portable_linux_packages.yml` from docker-in-docker
execution (via `linux_portable_build.py` ‚Üí `linux_portable_build_in_container.sh`)
to using the native GitHub Actions `container:` directive. This aligns the
release workflow with how `build_portable_linux_artifacts.yml` (CI) already
works. It also adds a `--build-dir` CLI argument to `build_configure.py`.

**Net changes:** +38 lines, -25 lines across 2 files

---

## Overall Assessment

**‚ö†Ô∏è CHANGES REQUESTED** ‚Äî The migration direction is correct and follows
established patterns in the CI workflows. However, there are important gaps
in the migration that could cause regressions.

**Strengths:**

- Correct direction: removes docker-in-docker indirection, aligning with CI
  patterns used by `build_portable_linux_artifacts.yml`
- Eliminates the need for `Grant ownership over output directory` step (since
  everything now runs inside the container natively)
- Proper addition of `AWS_SHARED_CREDENTIALS_FILE` and volume mount for
  credentials, matching `build_portable_linux_python_packages.yml` pattern
- Container image SHA is preserved, ensuring reproducibility
- Good that `build_configure.py` is reused (DRY) instead of duplicating cmake
  configuration logic

**Issues:**

- Missing ccache setup ‚Äî confirmed by workflow run data (save cache 0s vs 51s)
- `-DBUILD_TESTING=ON` now explicit (was implicit via CMake default ‚Äî not a regression, but worth noting)
- Minor cleanup gaps (dead code)

---

## Detailed Review

### 1. `release_portable_linux_packages.yml`

#### ‚ö†Ô∏è IMPORTANT: Missing ccache configuration ‚Äî confirmed by workflow run data

The old `linux_portable_build_in_container.sh` explicitly configured ccache:

```bash
export CCACHE_DIR="$OUTPUT_DIR/caches/container/ccache"
export PIP_CACHE_DIR="$OUTPUT_DIR/caches/container/pip"
mkdir -p "$CCACHE_DIR"
mkdir -p "$PIP_CACHE_DIR"
```

This ensured that:
1. ccache writes to `$OUTPUT_DIR/caches/container/ccache`
2. The `actions/cache` step (which saves/restores `$OUTPUT_DIR/caches`) captures
   ccache data across runs

The new workflow does not set `CCACHE_DIR` or `PIP_CACHE_DIR`. Without these,
ccache writes to its default location (`~/.cache/ccache`), which is NOT within
the cached `$OUTPUT_DIR/caches` path.

**Evidence from workflow runs:**

Comparing the [PR run](https://github.com/ROCm/TheRock/actions/runs/22135636130/job/63986544342)
against the [main branch run](https://github.com/ROCm/TheRock/actions/runs/22029823145/job/63652994941)
for gfx94X-dcgpu:

| Step | PR run (container) | Main run (docker-in-docker) |
|------|-------------------|-----------------------------|
| Build Projects | 5h 47m | 3h 33m |
| Build Python Packages | 8m 29s | 6m 54s |
| Enable cache (restore) | 0s (miss) | 0s (miss) |
| **Save cache** | **0s (nothing saved!)** | **51s (saved data)** |

The Save cache step is the smoking gun: the main branch saved 51s of ccache
data for future runs, while the PR saved nothing ‚Äî `$OUTPUT_DIR/caches` was
empty because ccache was writing elsewhere. Both had cache misses on restore
(fresh builds), so the build time difference here is primarily attributable to
different VM sizes. But the broken save means future builds on the PR branch
will *never* benefit from ccache warm-up.

**Recommendation (for this PR):** Restore parity with the old container script
by adding `CCACHE_DIR` and `PIP_CACHE_DIR` as job-level env vars:

```yaml
env:
  CCACHE_DIR: ${{ github.workspace }}/output/caches/ccache
  PIP_CACHE_DIR: ${{ github.workspace }}/output/caches/pip
```

This keeps the ccache data within `$OUTPUT_DIR/caches` so the existing
`actions/cache` save/restore steps work correctly, matching the old behavior.

> **Note:** The CI workflow (`build_portable_linux_artifacts.yml`) uses a
> different, more sophisticated pattern: `setup_ccache.py` with
> `secondary_storage` pointing at a bazelremote server, with no `actions/cache`
> at all. Migrating the release workflow to that pattern is a separate effort
> (see Future Follow-up).

---

#### üí° SUGGESTION: `-DBUILD_TESTING=ON` is now explicit (not a regression)

The old `linux_portable_build_in_container.sh` did not pass `-DBUILD_TESTING`.
The new flow uses `build_configure.py`, which hardcodes it at line 106:
```python
"-DBUILD_TESTING=ON",
```

This is **not a regression** ‚Äî `BUILD_TESTING` defaults to `ON` via
`include(CTest)` in the top-level `CMakeLists.txt` (line 85), so the old flow
got the same behavior implicitly. However, what was implicit is now explicit,
which changes the surface area: if someone later changes the CMake default,
the explicit flag in `build_configure.py` would override it.

Worth being aware of, but no action needed for this PR.

---

#### üí° SUGGESTION: Dead `wait` command in Fetch sources

The `wait` command remains after removing the background `docker pull`:

```yaml
- name: Fetch sources
  timeout-minutes: 30
  run: |
    ./build_tools/fetch_sources.py --jobs 10
    wait  # <-- no background processes to wait for
```

**Recommendation:** Remove the `wait` command.

---

#### üí° SUGGESTION: Pass `--manylinux` explicitly for consistency

The CI workflow (`build_portable_linux_artifacts.yml`) passes `--manylinux`
explicitly:
```yaml
python3 build_tools/github_actions/build_configure.py --manylinux
```

The release workflow relies on the `MANYLINUX: 1` env var:
```yaml
python3 ./build_tools/github_actions/build_configure.py \
  --build-dir ${{ env.BUILD_DIR }}
```

While both work (the code checks both), explicit is better than implicit.

---

### 2. `build_tools/github_actions/build_configure.py`

#### ‚ö†Ô∏è IMPORTANT: `build_dir` removed from module scope creates inconsistency

The `build_dir` assignment was removed from module level:
```python
# Removed:
build_dir = os.getenv("BUILD_DIR")
```

And moved to `__main__`:
```python
build_dir = args.build_dir or os.getenv("BUILD_DIR")
```

This works because `if __name__ == "__main__"` is at module scope, so `build_dir`
becomes a module-level global. However:

1. **Inconsistency**: All other config variables (`cmake_preset`,
   `amdgpu_families`, `package_version`, `extra_cmake_options`, etc.) are still
   set at module level. `build_dir` now follows a different pattern.
2. **Fragile**: If `build_configure()` is ever called without going through
   `__main__` (e.g., imported from tests), it will get a `NameError`. The other
   module-level vars would be `None` but at least exist.

**Recommendation:** Keep `build_dir` at module level as the default, override
in `__main__` if CLI arg is provided:
```python
# Module level (keep):
build_dir = os.getenv("BUILD_DIR")

# In __main__ (add override):
if args.build_dir:
    build_dir = args.build_dir
```

---

#### üí° SUGGESTION: Consider refactoring all module-level globals

The entire module uses globals for configuration (`cmake_preset`,
`amdgpu_families`, etc.), which is fragile. This is a pre-existing issue,
but the `--build-dir` addition is an opportunity to start moving toward
passing configuration through function parameters or a config object.

This is out of scope for this PR but worth noting.

---

## Recommendations

### ‚ùå REQUIRED (Blocking):

None ‚Äî the build functions correctly as demonstrated by the test run.

### ‚úÖ Recommended:

1. **Add `CCACHE_DIR` and `PIP_CACHE_DIR` env vars** ‚Äî Point them within
   `$OUTPUT_DIR/caches` to match the old container script behavior. Without
   this, the `actions/cache` save/restore steps are ineffective (confirmed
   by workflow run data: save cache 0s vs 51s on main).
2. **Keep `build_dir` at module level** in `build_configure.py` for consistency
   with the other config variables, and override in `__main__` when CLI arg is
   provided.

### üí° Consider:

1. Remove dead `wait` command from Fetch sources step.
2. Pass `--manylinux` explicitly in the workflow's Configure step.
3. Remove redundant `BUILD_DIR` job-level env var if only the `--build-dir` CLI
   arg is needed (or vice versa ‚Äî use only the env var for consistency with CI).

### üìã Future Follow-up:

1. **Migrate release ccache to `setup_ccache.py`** ‚Äî The CI workflow uses
   `setup_ccache.py` with `secondary_storage` pointing at a bazelremote server
   (no `actions/cache` needed). The release workflow should adopt a similar
   pattern with proper cache isolation. Policy:
   - CI runs: permissive shared cache (current bazelremote setup)
   - Nightly releases: own isolated cache (local preset + `actions/cache`, or
     a dedicated bazelremote namespace)
   - Stable releases (rockrel repo): either no cache or own isolated cache
   - Update the TODO comments (`"We shouldn't be using a cache on actual
     release branches"`) in both `release_portable_linux_packages.yml` and
     `release_windows_packages.yml` to reflect this policy.
2. Refactor `build_configure.py` to pass configuration through function
   parameters instead of module-level globals.
4. Evaluate whether `linux_portable_build.py` and
   `linux_portable_build_in_container.sh` can be deprecated now that both CI
   and release workflows use native `container:`. They still serve a purpose
   for local developer use, but the CI usage path is removed.

---

## Testing Recommendations

1. **Verify ccache save** ‚Äî After adding ccache configuration, run the workflow
   and confirm the "Save cache" step takes >0s (indicating ccache data was
   captured). Then run again and confirm "Enable cache" restores it.
2. **Compare cmake configure flags** ‚Äî Run the old and new workflows with
   verbose cmake output and diff the cmake invocations to catch any missing
   flags.
3. **Verify artifact output** ‚Äî Compare the artifact directory structure and
   file list between old and new workflows to ensure nothing is missing.

---

## Conclusion

**Approval Status: ‚ö†Ô∏è CHANGES REQUESTED**

The migration from docker-in-docker to native `container:` is the right
direction and follows established patterns. The core build logic works
correctly as shown by the test run. One regression needs to be fixed to
maintain parity:

1. **ccache** ‚Äî Workflow run data confirms the cache save is broken (0s vs 51s
   on main). Fix: add `CCACHE_DIR` and `PIP_CACHE_DIR` env vars pointing
   within `$OUTPUT_DIR/caches` to match the old container script behavior.
   Migrating to the `setup_ccache.py` pattern is a separate follow-up.

With this fix, the PR achieves parity with the old behavior and will be
ready for merge.
