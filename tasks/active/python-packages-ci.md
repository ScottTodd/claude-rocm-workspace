---
repositories:
  - therock
---

# Build and Test ROCm Python Packages in CI

**Status:** In progress
**Priority:** P1 (High)
**Started:** 2026-01-21
**Issue:** https://github.com/ROCm/TheRock/issues/1559
**Branch:** `python-packages-upload`

## Overview

Build and test ROCm Python packages as part of CI, not just during PyTorch release builds. Issues like #1347 and #1552 have gone undetected because `rocm-sdk test` only runs during PyTorch release builds. This task uploads Python packages to S3 and adds a test workflow.

## Goals

- [ ] Upload Python packages from `build_portable_linux_python_packages.yml` to S3
- [ ] Upload Python packages from `build_windows_python_packages.yml` to S3
- [ ] Create pip-compatible index for each workflow run
- [x] Add `test_rocm_wheels.yml` workflow to test packages with `rocm-sdk test`
- [ ] (Future) Extend `setup_venv.py` to install from workflow run S3 paths
- [ ] (Future) Trigger dev PyTorch builds and tests

## Context

### Background

Current state:
- `build_portable_linux_python_packages.yml` and `build_windows_python_packages.yml` build packages but don't upload
- Windows workflow runs `rocm-sdk test` locally but Linux doesn't (Docker issues)
- Both workflows have `TODO(#1559)` comments for S3 upload
- `test_pytorch_wheels.yml` already installs packages from a pip index and runs tests

The desired workflow:
1. Build workflow builds Python packages
2. Build workflow creates pip index and uploads to S3
3. Test workflow installs from S3 pip index and runs `rocm-sdk test`

### Related Work

- **PR #3000:** `RunOutputRoot` class for CI run outputs paths (blocked on other PRs, proceeding without)
- **PR #3099:** `test_rocm_wheels.yml` workflow (merged)
- **Issue #3115:** Test failures discovered when running on GPU machines
- **PR #3116:** Workaround for test failures
- **PR #3119:** Fine-grained test coverage improvements
- **PR #3093:** `find_artifacts_for_commit.py` - auto-discover latest CI artifacts (from `artifacts-for-commit` task)
- **Task:** `run-outputs-layout.md` - defines S3 layout structure
- **Workflow:** `test_pytorch_wheels.yml` - pattern for testing wheels
- **Workflow:** `release_portable_linux_packages.yml` - has S3 upload steps to reference

### Directories/Files Involved

```
.github/workflows/build_portable_linux_python_packages.yml
.github/workflows/build_windows_python_packages.yml
.github/workflows/test_rocm_wheels.yml  # NEW
build_tools/github_actions/upload_python_packages.py  # NEW - upload script
build_tools/github_actions/post_build_upload.py       # Reference for upload patterns
build_tools/github_actions/github_actions_utils.py    # retrieve_bucket_info()
build_tools/_therock_utils/run_outputs.py             # Future: RunOutputRoot
build_tools/setup_venv.py
build_tools/third_party/s3_management/   # existing tooling for deps index
docs/packaging/python_packaging.md
```

## Design

### S3 Layout for Python Packages

Two components: a **deps-only base index** (architecture-neutral, stable) and
**per-run indexes** (rocm packages only, per workflow run).

#### Base Index (deps only)

```
s3://therock-ci-artifacts/
└── python-deps/
    └── simple/                    # PEP 503 pip index
        ├── index.html
        ├── numpy/
        │   └── index.html
        ├── packaging/
        │   └── index.html
        └── ...                    # NO rocm packages
```

- Architecture-neutral (same deps for all GPU families)
- Managed by existing `update_dependencies.py` / `manage.py` tooling
- Contains only dependencies, never rocm packages

#### Per-Run Index (rocm packages only)

Aligned with `RunOutputRoot` layout:

```
s3://therock-ci-artifacts/
└── {run_id}-{platform}/
    └── python/{artifact_group}/
        ├── dist/
        │   ├── rocm-7.10.0.dev0.tar.gz
        │   ├── rocm_sdk_core-7.10.0.dev0-py3-none-linux_x86_64.whl
        │   ├── rocm_sdk_devel-7.10.0.dev0-py3-none-linux_x86_64.whl
        │   ├── rocm_sdk_libraries_gfx94X-7.10.0.dev0-py3-none-linux_x86_64.whl
        │   └── ...
        └── simple/               # pip index (generated by piprepo)
            ├── index.html
            ├── rocm/
            │   └── index.html
            ├── rocm-sdk-core/
            │   └── index.html
            └── ...               # ONLY rocm packages, no deps
```

### Pip Index Strategy

**Decision:** Deps-only base index + per-run rocm-only index.

**Rationale:** Version resolution. If we used `--extra-index-url=pypi.org` or
the nightly index, pip would see multiple versions of rocm packages and might
pick the wrong one. By having deps in one index and rocm packages in another,
pip can only find rocm packages in the per-run index.

**Installation:**

```bash
pip install rocm[libraries,devel] \
  --index-url=https://therock-ci-artifacts.s3.amazonaws.com/python-deps/simple/ \
  --extra-index-url=https://therock-ci-artifacts.s3.amazonaws.com/{run_id}-{platform}/python/{artifact_group}/simple/
```

**Alternatives considered:**

1. **Per-run index only + pypi.org** - Version ambiguity: pip might pick pypi's
   packages over our dev builds
2. **Per-run index only + nightly index** - Version ambiguity: pip might pick
   nightly's rocm packages over fresh dev builds
3. **Duplicate deps in every per-run index** - Wasteful, slower uploads

### RunOutputRoot Extensions

Add to `run_outputs.py`:

```python
# Python packages
def python_prefix(self, artifact_group: str) -> str:
    """S3 key prefix for Python packages directory."""
    return f"{self.prefix}/python/{artifact_group}"

def python_dist_prefix(self, artifact_group: str) -> str:
    """S3 key prefix for wheel/sdist files."""
    return f"{self.python_prefix(artifact_group)}/dist"

def python_index_prefix(self, artifact_group: str) -> str:
    """S3 key prefix for pip index (simple/ directory)."""
    return f"{self.python_prefix(artifact_group)}/simple"

def python_index_url(self, artifact_group: str) -> str:
    """Public URL for pip --index-url."""
    return f"{self.https_url}/python/{artifact_group}/simple/"
```

### Test Workflow Design

`test_rocm_wheels.yml`:

```yaml
inputs:
  amdgpu_family:
    description: GPU family to test
    type: string
  test_runs_on:
    description: Runner with GPU
    type: string
  package_run_id:
    description: Workflow run ID containing packages
    type: string
  package_version:
    description: Package version to install
    type: string
  artifact_group:
    description: Artifact group for index URL
    type: string

steps:
  - name: Set up venv and install packages
    run: |
      python build_tools/setup_venv.py .venv \
        --packages "rocm[libraries,devel]==${{ inputs.package_version }}" \
        --index-url "https://therock-ci-artifacts.s3.amazonaws.com/.../simple/" \
        --activate-in-future-github-actions-steps

  - name: Run rocm-sdk sanity tests
    run: rocm-sdk test
```

### setup_venv.py Extension (Future)

Add `--from-run-id` option:

```bash
python setup_venv.py .venv \
  --packages "rocm[libraries,devel]" \
  --from-run-id 12345678901 \
  --artifact-group gfx94X-dcgpu
```

This would compute the index URL automatically using `RunOutputRoot`.

## Investigation Notes

### 2026-01-21 - Initial Analysis

**Current workflow structure:**
- Both Linux and Windows workflows:
  1. Fetch artifacts from S3 (from a build workflow run)
  2. Build Python packages with `build_python_packages.py`
  3. Inspect packages (ls)
  4. Windows: sanity check with `piprepo build` + `pip install` + `rocm-sdk test`
  5. Linux: sanity check commented out (Docker issues)
  6. TODO: upload to S3

**Key observations:**
- `piprepo build` creates the pip-compatible index structure
- Need to upload both `dist/` (wheels/sdists) and `simple/` (index) directories
- Windows workflow already validates packages work locally
- Linux Docker container may need different approach for local testing

### 2026-01-26 - Created test_rocm_wheels.yml

Created `.github/workflows/test_rocm_wheels.yml` - a simplified version of `test_pytorch_wheels.yml`
that just installs ROCm packages and runs `rocm-sdk test`.

**Design decisions:**
- Uses existing `setup_venv.py` with `--index-url` and `--index-subdir` (reuses existing tooling)
- Deferred the "two-index pattern" - can test with nightly/dev release URLs for now
- Supports both `workflow_dispatch` (manual trigger) and `workflow_call` (can be called by other workflows)
- Uses same container image as `test_pytorch_wheels.yml` for Linux GPU runners
- Separate workflow file (vs inlining into build workflows) for better composability
  - Build workflows run on CPU runners; test needs GPU runners
  - Linux build can't run tests inline due to Docker issues
  - Separate workflow can test wheels from any source (nightly, dev, future CI uploads)

**Inputs:**
- `amdgpu_family` - GPU family (default: gfx94X-dcgpu)
- `test_runs_on` - Runner label (default: linux-mi325-1gpu-ossci-rocm-frac)
- `package_index_url` - Base URL (default: https://rocm.nightlies.amd.com/v2)
- `rocm_version` - Required, e.g., "7.10.0a20251124"
- `python_version` - Python version (default: 3.12)

**PR #3099** submitted for review.

### 2026-01-26 - S3 Upload Strategy Discussion

PR #3000 (`RunOutputRoot`) is blocked waiting on other PRs. Need to push ahead with S3
uploads without that refactoring.

**Key decision:** Use CI artifacts bucket, NOT dev releases bucket.
- Dev releases (`therock-dev-python`) are for actual dev releases, not CI testing
- CI builds should go to `therock-ci-artifacts` with per-run paths

**Upload location:**
```
Bucket: therock-ci-artifacts
Path:   {run_id}-linux/python/{artifact_group}/dist/
```

**Index generation options:**

1. **No index (shortcut):** Upload wheels only, download before installing
   ```bash
   # In test workflow:
   aws s3 sync s3://therock-ci-artifacts/{run_id}-linux/python/{artifact_group}/dist/ ./wheels/
   pip install rocm[libraries,devel]==$VERSION --find-links=./wheels/ --no-index
   ```

2. **Inline piprepo (cleaner):** Generate index during upload
   ```bash
   # In build workflow:
   piprepo build "$PACKAGES_DIR/dist"
   aws s3 sync $PACKAGES_DIR/dist/ s3://therock-ci-artifacts/.../dist/
   aws s3 sync $PACKAGES_DIR/dist/simple/ s3://therock-ci-artifacts/.../simple/
   ```
   Then test workflow uses `--index-url` as designed.

Option 2 isn't much more work since `piprepo` is already installed in the build workflow.
The index is just HTML files - small and fast to upload.

**Open questions:**
- IAM permissions for CI artifacts bucket (may need different role than `therock-dev`)
- Lifecycle policies for cleanup of old CI uploads

**Next:** Wait for PR review, then test workflow manually with nightly wheels after merge.

### 2026-01-27 - PR #3099 Merged, Test Failures Found

**PR #3099 merged.** Ran the test workflow on GPU machines and discovered test failures.

**Issue #3115:** Documents the test failures found when running `rocm-sdk test` on actual GPU hardware.

**Follow-up PRs:**
- **PR #3116:** Workaround for the immediate test failures (unblocks CI)
- **PR #3119:** Adds more fine-grained test coverage to catch issues earlier

This validates the approach - the test workflow successfully caught issues that weren't being
detected before. The follow-up work improves test granularity so failures are easier to diagnose.

### 2026-01-27 - S3 Upload Architecture Discussion

**Unified vision for S3 uploads:**
- Every workflow run uploads ALL files (logs, artifacts, packages) to `therock-*-artifacts` buckets
- Release workflows copy subsets to distribution buckets (e.g., `therock-nightly-python`)
- This unifies CI and CD - artifacts bucket is "source of truth", distribution buckets are curated views

**Existing infrastructure:**
- `post_build_upload.py` - uploads logs, artifacts, manifests from build workflows
- Uses `retrieve_bucket_info()` to select bucket based on repo/fork/release type
- Path pattern: `{external_repo}{run_id}-{platform}/`
- PR #3000 (`RunOutputRoot`) consolidates path computation but is blocked

**PyTorch wheels workflow comparison:**
- Uses dedicated Python buckets: `therock-{release_type}-python`
- Uploads to `s3://${S3_BUCKET_PY}/${s3_staging_subdir}/${amdgpu_family}/`
- Uses `manage.py` to regenerate pip index after upload
- Has staging → test → promote flow

**Decision: Create new `upload_python_packages.py` script**

Considered three options:

| Option | Description | Verdict |
|--------|-------------|---------|
| A. Extend `post_build_upload.py` | Add `--upload-python-packages` flag | Awkward - different input structure |
| B. New `upload_python_packages.py` | Separate script, same abstractions | **Selected** - clean separation |
| C. Refactor to unified `run_output_upload.py` | One script with modes | Larger scope, blocked on PR #3000 |

**Rationale for Option B:**
- `post_build_upload.py` expects build dir structure (`logs/`, `artifacts/`, manifest path)
- Python packages workflow has different structure (`PACKAGES_DIR/dist/`)
- New script can use same bucket selection logic (`retrieve_bucket_info()`)
- Can migrate to `RunOutputRoot` when PR #3000 lands
- Both scripts write to unified structure in artifacts bucket

**Script responsibilities:**
1. Use `retrieve_bucket_info()` to get bucket
2. Generate pip index with `piprepo build`
3. Upload wheels/sdists to `{run_id}-{platform}/python/{artifact_group}/dist/`
4. Upload pip index to `{run_id}-{platform}/python/{artifact_group}/simple/`
5. Add links to GitHub job summary

**Local testing support:**
- `--dry-run` flag: print what would be uploaded without actually uploading
- `--output-dir` flag: "upload" to local directory instead of S3
- `--bucket` override: can test with `therock-artifacts-testing` bucket
- Useful for validating path computation and index generation locally

**Committed:** `e02e888` on branch `python-packages-upload` - initial script without piprepo integration

### Testing Procedure (TODO: Document)

Need to develop and document a full local testing workflow:

1. **Fetch artifacts from CI**
   ```bash
   RUN_ID=17123441166
   TARGET=gfx110X-all
   mkdir -p $HOME/.therock/$RUN_ID/artifacts
   python ./build_tools/fetch_artifacts.py \
     --run-id=$RUN_ID \
     --target=$TARGET \
     --output-dir=$HOME/.therock/$RUN_ID/artifacts
   ```

2. **Build Python packages**
   ```bash
   python ./build_tools/build_python_packages.py \
     --artifact-dir=$HOME/.therock/$RUN_ID/artifacts \
     --dest-dir=$HOME/.therock/$RUN_ID/packages
   ```

3. **Run upload script with local output**
   ```bash
   python ./build_tools/github_actions/upload_python_packages.py \
     --packages-dir=$HOME/.therock/$RUN_ID/packages \
     --artifact-group=$TARGET \
     --run-id=$RUN_ID \
     --output-dir=$HOME/.therock/$RUN_ID/upload-test
   ```

4. **Test install from local output**
   ```bash
   # Generate pip index (TODO: will be done by upload script)
   pip install piprepo
   piprepo build $HOME/.therock/$RUN_ID/upload-test/$RUN_ID-linux/python/$TARGET/dist

   # Install from local index
   python -m venv .venv && source .venv/bin/activate
   pip install rocm[libraries,devel] --pre \
     --extra-index-url file://$HOME/.therock/$RUN_ID/upload-test/$RUN_ID-linux/python/$TARGET/dist/simple
   rocm-sdk test
   ```

**Reference docs:**
- `external-builds/pytorch/README.md` - "Building the rocm Python packages from artifacts fetched from a CI run"
- `docs/packaging/python_packaging.md` - piprepo usage for local index

**Related:** PR #3093 (`artifacts-for-commit` task) will add `find_artifacts_for_commit.py` to get
latest CI artifacts without manually looking up run IDs. Once that lands, step 1 becomes:
```bash
# Auto-discover latest artifacts for a commit/branch
python ./build_tools/find_artifacts_for_commit.py --target=gfx110X-all --output-dir=...
```

**TODO:** Once workflow is validated:
- Add to `docs/packaging/python_packaging.md` or create new dev guide
- Include workflow diagrams showing the pipeline (local dev + CI paths)
- Diagram format: Excalidraw SVGs for complex diagrams (see `docs/development/assets/*.excalidraw.svg`),
  mermaid.js or ASCII for simpler inline diagrams
- Reference: `docs/development/development_guide.md` has existing build architecture diagram

## Implementation Plan

### Phase 1: S3 Upload from Build Workflows

1. **Create `upload_python_packages.py`** script:
   - Use `retrieve_bucket_info()` for bucket selection
   - Generate pip index with `piprepo build`
   - Upload wheels/sdists to `{run_id}-{platform}/python/{artifact_group}/dist/`
   - Upload pip index to `{run_id}-{platform}/python/{artifact_group}/simple/`
   - Add links to GitHub job summary
   - (Future: migrate to `RunOutputRoot` when PR #3000 lands)
2. **Update Linux workflow** to call upload script
3. **Update Windows workflow** to call upload script (same pattern)
4. **Manual testing:** Download packages from S3, install manually, verify they work

### Phase 2: Base Deps Index Setup

1. **Create `python-deps/` in `therock-ci-artifacts`** bucket
2. **Run `update_dependencies.py`** to mirror deps (reuse existing tooling)
3. **Run `manage.py`** to generate index
4. **Verify** deps are accessible at expected URL

### Phase 3: Test Workflow

1. **Create `test_rocm_wheels.yml`**:
   - Similar to `test_pytorch_wheels.yml` but simpler (no PyTorch checkout)
   - Input: workflow run ID, artifact group, package version
   - Install packages from S3 pip index (using two-index pattern)
   - Run `rocm-sdk test`
2. **Add workflow_call support** so build workflows can trigger tests

### Phase 4: Integration

1. **Have build workflows call test workflow** after successful upload
2. **(Future) Extend `setup_venv.py`** with `--from-run-id` option

## Dependencies

- **PR #3000** (`RunOutputRoot`) - nice to have but not blocking; can use `retrieve_bucket_info()` directly
- PR #3019 (`fetch_artifacts.py` refactor) - not blocking

## Next Steps

1. [x] Create task file
2. [x] Create test workflow (`test_rocm_wheels.yml`) - **PR #3099** merged
3. [ ] Create `upload_python_packages.py` script
   - Use `retrieve_bucket_info()` for bucket selection
   - Generate pip index with `piprepo build`
   - Upload dist/ and simple/ to S3
   - Add GitHub job summary links
4. [ ] Add S3 upload step to `build_portable_linux_python_packages.yml`
   - Add `id-token: write` permission for AWS credentials
   - Call `upload_python_packages.py`
5. [ ] Update `test_rocm_wheels.yml` to accept CI artifact URLs
6. [ ] Test end-to-end with CI-built wheels
7. [ ] Add S3 upload to `build_windows_python_packages.yml` (same pattern)

## Decisions Made

- **Bucket:** CI artifacts bucket (`therock-ci-artifacts`) for CI builds, NOT dev releases bucket
  - Path: `{run_id}-linux/python/{artifact_group}/` for per-run isolation
  - Keeps CI builds separate from actual dev/nightly/prerelease releases
- **Upload script:** New `upload_python_packages.py` (not extending `post_build_upload.py`)
  - Different input structure (packages dir vs build dir with logs/artifacts/manifests)
  - Uses same bucket selection logic (`retrieve_bucket_info()`)
  - Can migrate to `RunOutputRoot` when PR #3000 lands
- **Index strategy (target):** Deps-only base index + per-run rocm-only index (avoids version ambiguity)
- **Index strategy (shortcut):** Can skip index generation initially, use `--find-links` with downloaded wheels
- **Base index:** Architecture-neutral, reuse existing `update_dependencies.py` / `manage.py` tooling
- **Linux sanity check:** Skip local testing in build workflow, rely on GPU test workflow
- **Test workflow:** Separate `test_rocm_wheels.yml` (not inlined into build workflows)
  - Build workflows use CPU runners; testing needs GPU runners
  - Separate workflow enables testing wheels from any source (CI, nightly, dev)

## Open Questions

- Should test workflow run on PRs or only after merge?
  - **Leaning:** At least on merge; optionally on PRs with GPU runner availability
- Which deps to include in base index? Same as nightly, or a subset?
- IAM permissions for CI artifacts bucket - does existing role work or need new one?
- Lifecycle policies for CI uploads - how long to retain per-run packages?
