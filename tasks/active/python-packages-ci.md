---
repositories:
  - therock
---

# Build and Test ROCm Python Packages in CI

**Status:** In progress
**Priority:** P1 (High)
**Started:** 2026-01-21
**Issue:** https://github.com/ROCm/TheRock/issues/1559
**Branch:** `python-packages-upload`

## Overview

Build and test ROCm Python packages as part of CI, not just during PyTorch release builds. Issues like #1347 and #1552 have gone undetected because `rocm-sdk test` only runs during PyTorch release builds. This task uploads Python packages to S3 and adds a test workflow.

## Goals

- [ ] Upload Python packages from `build_portable_linux_python_packages.yml` to S3
- [ ] Upload Python packages from `build_windows_python_packages.yml` to S3
- [ ] Create pip-compatible index for each workflow run
- [x] Add `test_rocm_wheels.yml` workflow to test packages with `rocm-sdk test`
- [ ] (Future) Extend `setup_venv.py` to install from workflow run S3 paths
- [ ] (Future) Trigger dev PyTorch builds and tests

## Context

### Background

Current state:
- `build_portable_linux_python_packages.yml` and `build_windows_python_packages.yml` build packages but don't upload
- Windows workflow runs `rocm-sdk test` locally but Linux doesn't (Docker issues)
- Both workflows have `TODO(#1559)` comments for S3 upload
- `test_pytorch_wheels.yml` already installs packages from a pip index and runs tests

The desired workflow:
1. Build workflow builds Python packages
2. Build workflow creates pip index and uploads to S3
3. Test workflow installs from S3 pip index and runs `rocm-sdk test`

### Related Work

- **Issue #3177:** Tracking issue for expanding CI workflows (ROCm Python, PyTorch, JAX, native Linux)
- **PR #3182:** E2E integration - upload and test Python packages in CI (draft, awaiting results)
- **PR #3000:** `RunOutputRoot` class for CI run outputs paths (blocked on other PRs, proceeding without)
- **PR #3099:** `test_rocm_wheels.yml` workflow (merged)
- **Issue #3115:** Test failures discovered when running on GPU machines
- **PR #3116:** Workaround for test failures
- **PR #3119:** Fine-grained test coverage improvements
- **PR #3093:** `find_artifacts_for_commit.py` - auto-discover latest CI artifacts (from `artifacts-for-commit` task)
- **PR #3136:** `upload_python_packages.py` script and documentation (merged)
- **PR #3214:** Container migration for Linux Python packages workflow (merged)
- **PR #3233:** Add timeouts to `test_rocm_wheels.yml` (merged)
- **PR #3242:** setup_venv.py refactoring - `--find-links`, `--pre`, improved tests (in review)
- **PR #3261:** Upload and test Python packages in CI workflows (draft, stacked on #3242)
- **Task:** `run-outputs-layout.md` - defines S3 layout structure
- **Workflow:** `test_pytorch_wheels.yml` - pattern for testing wheels
- **Workflow:** `release_portable_linux_packages.yml` - has S3 upload steps to reference

### Directories/Files Involved

```
.github/workflows/build_portable_linux_python_packages.yml
.github/workflows/build_windows_python_packages.yml
.github/workflows/test_rocm_wheels.yml  # NEW
build_tools/github_actions/upload_python_packages.py  # NEW - upload script
build_tools/github_actions/post_build_upload.py       # Reference for upload patterns
build_tools/github_actions/github_actions_utils.py    # retrieve_bucket_info()
build_tools/_therock_utils/run_outputs.py             # Future: RunOutputRoot
build_tools/setup_venv.py
build_tools/third_party/s3_management/   # existing tooling for deps index
docs/packaging/python_packaging.md
```

## Design

### S3 Layout for Python Packages

Two components: a **deps-only base index** (architecture-neutral, stable) and
**per-run indexes** (rocm packages only, per workflow run).

#### Base Index (deps only)

```
s3://therock-ci-artifacts/
└── python-deps/
    └── simple/                    # PEP 503 pip index
        ├── index.html
        ├── numpy/
        │   └── index.html
        ├── packaging/
        │   └── index.html
        └── ...                    # NO rocm packages
```

- Architecture-neutral (same deps for all GPU families)
- Managed by existing `update_dependencies.py` / `manage.py` tooling
- Contains only dependencies, never rocm packages

#### Per-Run Index (rocm packages only)

Aligned with `RunOutputRoot` layout:

```
s3://therock-ci-artifacts/
└── {run_id}-{platform}/
    └── python/{artifact_group}/
        ├── rocm-7.10.0.dev0.tar.gz
        ├── rocm_sdk_core-7.10.0.dev0-py3-none-linux_x86_64.whl
        ├── rocm_sdk_devel-7.10.0.dev0-py3-none-linux_x86_64.whl
        ├── rocm_sdk_libraries_gfx94X-7.10.0.dev0-py3-none-linux_x86_64.whl
        └── index.html            # flat file listing for pip --find-links
```

**Note:** Originally planned to use piprepo with `simple/` PEP-503 structure, but
switched to flat `index.html` generated by `indexer.py` because:
- S3 doesn't serve `index.html` for directory requests (no static website hosting)
- `--find-links` works with flat index, `--index-url` requires PEP-503 structure
- `indexer.py` already exists in repo, piprepo is unmaintained

### Pip Index Strategy

**Decision:** Deps-only base index + per-run rocm-only index.

**Rationale:** Version resolution. If we used `--extra-index-url=pypi.org` or
the nightly index, pip would see multiple versions of rocm packages and might
pick the wrong one. By having deps in one index and rocm packages in another,
pip can only find rocm packages in the per-run index.

**Installation:**

```bash
pip install rocm[libraries,devel] --pre \
  --find-links=https://therock-ci-artifacts.s3.amazonaws.com/{run_id}-{platform}/python/{artifact_group}/index.html
```

For deps-only base index (future):
```bash
pip install rocm[libraries,devel] --pre \
  --index-url=https://therock-ci-artifacts.s3.amazonaws.com/python-deps/simple/ \
  --find-links=https://therock-ci-artifacts.s3.amazonaws.com/{run_id}-{platform}/python/{artifact_group}/index.html
```

**Alternatives considered:**

1. **Per-run index only + pypi.org** - Version ambiguity: pip might pick pypi's
   packages over our dev builds
2. **Per-run index only + nightly index** - Version ambiguity: pip might pick
   nightly's rocm packages over fresh dev builds
3. **Duplicate deps in every per-run index** - Wasteful, slower uploads

### RunOutputRoot Extensions

Add to `run_outputs.py`:

```python
# Python packages
def python_prefix(self, artifact_group: str) -> str:
    """S3 key prefix for Python packages directory."""
    return f"{self.prefix}/python/{artifact_group}"

def python_dist_prefix(self, artifact_group: str) -> str:
    """S3 key prefix for wheel/sdist files."""
    return f"{self.python_prefix(artifact_group)}/dist"

def python_index_prefix(self, artifact_group: str) -> str:
    """S3 key prefix for pip index (simple/ directory)."""
    return f"{self.python_prefix(artifact_group)}/simple"

def python_index_url(self, artifact_group: str) -> str:
    """Public URL for pip --index-url."""
    return f"{self.https_url}/python/{artifact_group}/simple/"
```

### Test Workflow Design

`test_rocm_wheels.yml`:

```yaml
inputs:
  amdgpu_family:
    description: GPU family to test
    type: string
  test_runs_on:
    description: Runner with GPU
    type: string
  package_run_id:
    description: Workflow run ID containing packages
    type: string
  package_version:
    description: Package version to install
    type: string
  artifact_group:
    description: Artifact group for index URL
    type: string

steps:
  - name: Set up venv and install packages
    run: |
      python build_tools/setup_venv.py .venv \
        --packages "rocm[libraries,devel]==${{ inputs.package_version }}" \
        --index-url "https://therock-ci-artifacts.s3.amazonaws.com/.../simple/" \
        --activate-in-future-github-actions-steps

  - name: Run rocm-sdk sanity tests
    run: rocm-sdk test
```

### setup_venv.py Extension (Future)

Add `--from-run-id` option:

```bash
python setup_venv.py .venv \
  --packages "rocm[libraries,devel]" \
  --from-run-id 12345678901 \
  --artifact-group gfx94X-dcgpu
```

This would compute the index URL automatically using `RunOutputRoot`.

## Investigation Notes

### 2026-01-21 - Initial Analysis

**Current workflow structure:**
- Both Linux and Windows workflows:
  1. Fetch artifacts from S3 (from a build workflow run)
  2. Build Python packages with `build_python_packages.py`
  3. Inspect packages (ls)
  4. Windows: sanity check with `piprepo build` + `pip install` + `rocm-sdk test`
  5. Linux: sanity check commented out (Docker issues)
  6. TODO: upload to S3

**Key observations:**
- `piprepo build` creates the pip-compatible index structure
- Need to upload both `dist/` (wheels/sdists) and `simple/` (index) directories
- Windows workflow already validates packages work locally
- Linux Docker container may need different approach for local testing

### 2026-01-26 - Created test_rocm_wheels.yml

Created `.github/workflows/test_rocm_wheels.yml` - a simplified version of `test_pytorch_wheels.yml`
that just installs ROCm packages and runs `rocm-sdk test`.

**Design decisions:**
- Uses existing `setup_venv.py` with `--index-url` and `--index-subdir` (reuses existing tooling)
- Deferred the "two-index pattern" - can test with nightly/dev release URLs for now
- Supports both `workflow_dispatch` (manual trigger) and `workflow_call` (can be called by other workflows)
- Uses same container image as `test_pytorch_wheels.yml` for Linux GPU runners
- Separate workflow file (vs inlining into build workflows) for better composability
  - Build workflows run on CPU runners; test needs GPU runners
  - Linux build can't run tests inline due to Docker issues
  - Separate workflow can test wheels from any source (nightly, dev, future CI uploads)

**Inputs:**
- `amdgpu_family` - GPU family (default: gfx94X-dcgpu)
- `test_runs_on` - Runner label (default: linux-mi325-1gpu-ossci-rocm-frac)
- `package_index_url` - Base URL (default: https://rocm.nightlies.amd.com/v2)
- `rocm_version` - Required, e.g., "7.10.0a20251124"
- `python_version` - Python version (default: 3.12)

**PR #3099** submitted for review.

### 2026-01-26 - S3 Upload Strategy Discussion

PR #3000 (`RunOutputRoot`) is blocked waiting on other PRs. Need to push ahead with S3
uploads without that refactoring.

**Key decision:** Use CI artifacts bucket, NOT dev releases bucket.
- Dev releases (`therock-dev-python`) are for actual dev releases, not CI testing
- CI builds should go to `therock-ci-artifacts` with per-run paths

**Upload location:**
```
Bucket: therock-ci-artifacts
Path:   {run_id}-linux/python/{artifact_group}/dist/
```

**Index generation options:**

1. **No index (shortcut):** Upload wheels only, download before installing
   ```bash
   # In test workflow:
   aws s3 sync s3://therock-ci-artifacts/{run_id}-linux/python/{artifact_group}/dist/ ./wheels/
   pip install rocm[libraries,devel]==$VERSION --find-links=./wheels/ --no-index
   ```

2. **Inline piprepo (cleaner):** Generate index during upload
   ```bash
   # In build workflow:
   piprepo build "$PACKAGES_DIR/dist"
   aws s3 sync $PACKAGES_DIR/dist/ s3://therock-ci-artifacts/.../dist/
   aws s3 sync $PACKAGES_DIR/dist/simple/ s3://therock-ci-artifacts/.../simple/
   ```
   Then test workflow uses `--index-url` as designed.

Option 2 isn't much more work since `piprepo` is already installed in the build workflow.
The index is just HTML files - small and fast to upload.

**Open questions:**
- IAM permissions for CI artifacts bucket (may need different role than `therock-dev`)
- Lifecycle policies for cleanup of old CI uploads

**Next:** Wait for PR review, then test workflow manually with nightly wheels after merge.

### 2026-01-27 - PR #3099 Merged, Test Failures Found

**PR #3099 merged.** Ran the test workflow on GPU machines and discovered test failures.

**Issue #3115:** Documents the test failures found when running `rocm-sdk test` on actual GPU hardware.

**Follow-up PRs:**
- **PR #3116:** Workaround for the immediate test failures (unblocks CI)
- **PR #3119:** Adds more fine-grained test coverage to catch issues earlier

This validates the approach - the test workflow successfully caught issues that weren't being
detected before. The follow-up work improves test granularity so failures are easier to diagnose.

### 2026-01-27 - S3 Upload Architecture Discussion

**Unified vision for S3 uploads:**
- Every workflow run uploads ALL files (logs, artifacts, packages) to `therock-*-artifacts` buckets
- Release workflows copy subsets to distribution buckets (e.g., `therock-nightly-python`)
- This unifies CI and CD - artifacts bucket is "source of truth", distribution buckets are curated views

**Existing infrastructure:**
- `post_build_upload.py` - uploads logs, artifacts, manifests from build workflows
- Uses `retrieve_bucket_info()` to select bucket based on repo/fork/release type
- Path pattern: `{external_repo}{run_id}-{platform}/`
- PR #3000 (`RunOutputRoot`) consolidates path computation but is blocked

**PyTorch wheels workflow comparison:**
- Uses dedicated Python buckets: `therock-{release_type}-python`
- Uploads to `s3://${S3_BUCKET_PY}/${s3_staging_subdir}/${amdgpu_family}/`
- Uses `manage.py` to regenerate pip index after upload
- Has staging → test → promote flow

**Decision: Create new `upload_python_packages.py` script**

Considered three options:

| Option | Description | Verdict |
|--------|-------------|---------|
| A. Extend `post_build_upload.py` | Add `--upload-python-packages` flag | Awkward - different input structure |
| B. New `upload_python_packages.py` | Separate script, same abstractions | **Selected** - clean separation |
| C. Refactor to unified `run_output_upload.py` | One script with modes | Larger scope, blocked on PR #3000 |

**Rationale for Option B:**
- `post_build_upload.py` expects build dir structure (`logs/`, `artifacts/`, manifest path)
- Python packages workflow has different structure (`PACKAGES_DIR/dist/`)
- New script can use same bucket selection logic (`retrieve_bucket_info()`)
- Can migrate to `RunOutputRoot` when PR #3000 lands
- Both scripts write to unified structure in artifacts bucket

**Script responsibilities:**
1. Use `retrieve_bucket_info()` to get bucket
2. Generate pip index with `piprepo build`
3. Upload wheels/sdists to `{run_id}-{platform}/python/{artifact_group}/dist/`
4. Upload pip index to `{run_id}-{platform}/python/{artifact_group}/simple/`
5. Add links to GitHub job summary

**Local testing support:**
- `--dry-run` flag: print what would be uploaded without actually uploading
- `--output-dir` flag: "upload" to local directory instead of S3
- `--bucket` override: can test with `therock-artifacts-testing` bucket
- Useful for validating path computation and index generation locally

**Committed:** `e02e888` on branch `python-packages-upload` - initial script without piprepo integration

### Testing Procedure (TODO: Document)

Need to develop and document a full local testing workflow:

1. **Fetch artifacts from CI**
   ```bash
   RUN_ID=17123441166
   TARGET=gfx110X-all
   mkdir -p $HOME/.therock/$RUN_ID/artifacts
   python ./build_tools/fetch_artifacts.py \
     --run-id=$RUN_ID \
     --target=$TARGET \
     --output-dir=$HOME/.therock/$RUN_ID/artifacts
   ```

2. **Build Python packages**
   ```bash
   python ./build_tools/build_python_packages.py \
     --artifact-dir=$HOME/.therock/$RUN_ID/artifacts \
     --dest-dir=$HOME/.therock/$RUN_ID/packages
   ```

3. **Run upload script with local output**
   ```bash
   python ./build_tools/github_actions/upload_python_packages.py \
     --packages-dir=$HOME/.therock/$RUN_ID/packages \
     --artifact-group=$TARGET \
     --run-id=$RUN_ID \
     --output-dir=$HOME/.therock/$RUN_ID/upload-test
   ```

4. **Test install from local output**
   ```bash
   # Generate pip index (TODO: will be done by upload script)
   pip install piprepo
   piprepo build $HOME/.therock/$RUN_ID/upload-test/$RUN_ID-linux/python/$TARGET/dist

   # Install from local index
   python -m venv .venv && source .venv/bin/activate
   pip install rocm[libraries,devel] --pre \
     --extra-index-url file://$HOME/.therock/$RUN_ID/upload-test/$RUN_ID-linux/python/$TARGET/dist/simple
   rocm-sdk test
   ```

**Reference docs:**
- `external-builds/pytorch/README.md` - "Building the rocm Python packages from artifacts fetched from a CI run"
- `docs/packaging/python_packaging.md` - piprepo usage for local index

**Related:** PR #3093 (`artifacts-for-commit` task) will add `find_artifacts_for_commit.py` to get
latest CI artifacts without manually looking up run IDs. Once that lands, step 1 becomes:
```bash
# Auto-discover latest artifacts for a commit/branch
python ./build_tools/find_artifacts_for_commit.py --target=gfx110X-all --output-dir=...
```

**TODO:** Once workflow is validated:
- Add to `docs/packaging/python_packaging.md` or create new dev guide
- Include workflow diagrams showing the pipeline (local dev + CI paths)
- Diagram format: Excalidraw SVGs for complex diagrams (see `docs/development/assets/*.excalidraw.svg`),
  mermaid.js or ASCII for simpler inline diagrams
- Reference: `docs/development/development_guide.md` has existing build architecture diagram

### 2026-01-28 - Completed upload_python_packages.py

**PR #3136 submitted for review.**

Key implementation decisions:

1. **Replaced piprepo with `indexer.py`:**
   - piprepo is unmaintained (last updated 2018) and shows deprecation warnings
   - `third-party/indexer/indexer.py` already exists in the repo
   - Generates flat `index.html` for `--find-links` (not PEP-503 `simple/` structure)
   - Avoids S3 static website hosting requirement (S3 doesn't serve `index.html` for directory requests)

2. **S3 layout simplified:**
   ```
   {bucket}/{run_id}-{platform}/python/{artifact_group}/
     *.whl, *.tar.gz   # Package files
     index.html        # File listing for pip --find-links
   ```

3. **Installation uses `--find-links` instead of `--index-url`:**
   ```bash
   pip install rocm[libraries,devel] --pre \
     --find-links=https://{bucket}.s3.amazonaws.com/{path}/index.html
   ```

4. **`UploadPath` dataclass** added for S3 path computation:
   - Properties: `s3_uri` (for aws cli), `s3_url` (for pip/browser)
   - Will be consolidated with `RunOutputRoot` when PR #3000 lands

5. **Documentation updated:**
   - `docs/packaging/python_packaging.md` now has complete workflow for building from CI artifacts
   - Shows `fetch_artifacts.py` → `build_python_packages.py` → `indexer.py` → `pip install`

**Testing performed:**
- Downloaded artifacts from CI run 21440027240 (gfx110X-all)
- Built Python packages locally
- Generated index with upload script (`--output-dir` mode)
- Successfully installed with `pip install rocm --find-links=.../index.html`
- Uploaded to `therock-artifacts-testing` bucket and verified pip install works from S3

### 2026-01-29 - Integration and CI Test Run

**Branch:** `users/scotttodd/python-package-test`

**Changes made:**
1. Plumbed `--find-links-url` through `setup_venv.py` and `test_rocm_wheels.yml`
   - `setup_venv.py` now accepts `--find-links-url` as alternative to `--index-url`/`--index-subdir`
   - `test_rocm_wheels.yml` accepts `package_find_links_url` input
   - Both URL patterns can be passed; script prioritizes `--find-links-url` if set

2. Connected `build_portable_linux_python_packages.yml` to `test_rocm_wheels.yml`:
   - Added `generate_target_to_run` job (same pattern as other workflows)
   - Added `test_rocm_wheels` job that calls the test workflow
   - Upload script sets `package_find_links_url` output via `gha_set_output()`

3. Integration with `ci_linux.yml` is implicit:
   - `ci_linux.yml` → `build_portable_linux_python_packages.yml` → `test_rocm_wheels.yml`
   - No need to pass URL back up; testing happens within build workflow

**CI test run:** https://github.com/ROCm/TheRock/actions/runs/21499417722/job/61942129720

**Issues discovered:**
1. **AWS CLI not available:** The upload step runs outside the Docker container, so `aws` isn't on PATH
2. **AWS credentials not loaded:** Need proper IAM role configuration for the job

**Root cause:** The workflow uses `linux_portable_build.py --image=... --build-python-only` which runs
the build inside a Docker container, but the upload step runs on the host runner where AWS CLI and
credentials aren't available.

**Solution:** Revamp the Linux workflow to match `build_portable_linux_artifacts.yml` pattern:
- Run the entire job under the container image (using `container:` in the job spec)
- Call `build_python_packages.py` directly instead of going through `linux_portable_build.py`
- This ensures `aws` CLI is available and credentials are properly loaded
- Reference: `build_windows_python_packages.yml` already uses this pattern (no Docker wrapper)

### 2026-01-30 - Windows Workflow and Tracking Issue

**Commit `ed9b03ae`:** Added S3 upload and GPU testing to Windows workflow.

Changes to `build_windows_python_packages.yml`:
- Added upload step using `upload_python_packages.py`
- Added `generate_target_to_run` job to determine Windows test runner
- Added `test_rocm_wheels` job to run GPU tests after upload
- Added `repository`/`ref` inputs for workflow_call compatibility
- Removed inline sanity check (testing happens on GPU machines now)
- Removed `piprepo` dependency (upload script uses `indexer.py`)

This matches the pattern from the Linux workflow.

**Issue #3177 filed:** Tracking issue for expanding CI workflows to build and test packages.

Key points from the issue:
- CI and CD workflows have diverged; commits passing presubmit can break releases
- Goal: CI and CD workflows should reuse the same building blocks
- Central output directory per workflow run in cloud storage (`{run_id}-{platform}/`)
- Release workflows copy subsets to dedicated release buckets

Implementation plan from issue:
1. ROCm Python packages (this task) - upload + test in CI
2. PyTorch packages - rework staging/promote logic for CI vs release
3. Integrate into rocm-libraries and rocm-systems repos
4. JAX packages - either use tarballs or switch to ROCm Python packages
5. Native Linux packages - support CI artifacts bucket

Related issues: #1522, #2281

### 2026-01-30 (continued) - Linux Workflow Refactor and E2E Testing

**Windows workflow debugging:**
- First CI run failed with path mangling issue (`Path.resolve()` on Windows with Git Bash paths)
- Fixed by removing `.resolve()` call in `upload_python_packages.py` (commit `d99ced83`)
- Second CI run failed due to missing quotes around workflow arguments
- Fixed quoting in both Windows and Linux workflows

**Linux workflow refactored** to run entirely inside container:
- Added `container:` directive with manylinux image and AWS config volume mount
- Added `permissions: id-token: write` for AWS OIDC auth
- Added `AWS_SHARED_CREDENTIALS_FILE` env var
- Replaced `linux_portable_build.py --build-python-only` with direct `build_python_packages.py` call
- Added "Configure AWS Credentials" step before upload
- Removed unused `MANYLINUX` env var (only needed for cmake builds, not Python packaging)

**PR #3182 submitted:** Draft PR showing full e2e integration in CI workflows.
- Builds Python packages for all artifact groups
- Runs tests on GPU runners where available
- Waiting for workflow results

**Validation:** Used the new workflow to test PR #2877 (suspected to break Python packages).
The workflow failed as expected, proving this CI coverage is valuable.
See: https://github.com/ROCm/TheRock/pull/2877#discussion_r2748337856

**Issue #1559 updated** with progress notes.

## Implementation Plan

### Phase 1: S3 Upload from Build Workflows

1. **Create `upload_python_packages.py`** script:
   - Use `retrieve_bucket_info()` for bucket selection
   - Generate pip index with `piprepo build`
   - Upload wheels/sdists to `{run_id}-{platform}/python/{artifact_group}/dist/`
   - Upload pip index to `{run_id}-{platform}/python/{artifact_group}/simple/`
   - Add links to GitHub job summary
   - (Future: migrate to `RunOutputRoot` when PR #3000 lands)
2. **Update Linux workflow** to call upload script
3. **Update Windows workflow** to call upload script (same pattern)
4. **Manual testing:** Download packages from S3, install manually, verify they work

### Phase 2: Base Deps Index Setup

1. **Create `python-deps/` in `therock-ci-artifacts`** bucket
2. **Run `update_dependencies.py`** to mirror deps (reuse existing tooling)
3. **Run `manage.py`** to generate index
4. **Verify** deps are accessible at expected URL

### Phase 3: Test Workflow

1. **Create `test_rocm_wheels.yml`**:
   - Similar to `test_pytorch_wheels.yml` but simpler (no PyTorch checkout)
   - Input: workflow run ID, artifact group, package version
   - Install packages from S3 pip index (using two-index pattern)
   - Run `rocm-sdk test`
2. **Add workflow_call support** so build workflows can trigger tests

### Phase 4: Integration

1. **Have build workflows call test workflow** after successful upload
2. **(Future) Extend `setup_venv.py`** with `--from-run-id` option

## Dependencies

- **PR #3000** (`RunOutputRoot`) - nice to have but not blocking; can use `retrieve_bucket_info()` directly
- PR #3019 (`fetch_artifacts.py` refactor) - not blocking

## Next Steps

1. [x] Create task file
2. [x] Create test workflow (`test_rocm_wheels.yml`) - **PR #3099** merged
3. [x] Create `upload_python_packages.py` script - **PR #3136** merged
   - Uses `retrieve_bucket_info()` for bucket selection
   - Generates index with `indexer.py` (replaced piprepo)
   - Uploads to `{run_id}-{platform}/python/{artifact_group}/`
   - Adds GitHub job summary with install instructions
4. [x] Update `test_rocm_wheels.yml` to accept CI artifact URLs (use `--find-links`)
   - Added `package_find_links_url` input
   - Updated `setup_venv.py` with `--find-links` flag
5. [x] Connect `build_portable_linux_python_packages.yml` to `test_rocm_wheels.yml`
   - Initial integration done on branch `users/scotttodd/python-package-test`
   - CI test revealed AWS/Docker issues (see 2026-01-29 notes)
6. [x] **Port changes to Windows workflow** (`build_windows_python_packages.yml`)
   - Added upload step calling `upload_python_packages.py`
   - Added test workflow call with `package_find_links_url`
   - Removed inline sanity check (testing on GPU machines)
   - Commit `ed9b03ae` on branch `users/scotttodd/python-package-test`
7. [x] **Test Windows workflow** in CI
   - Fixed `Path.resolve()` issue (commit `d99ced83`)
   - Fixed argument quoting issues
8. [x] **Revamp Linux workflow**
   - Run entire job under container image (like `build_portable_linux_artifacts.yml`)
   - Call `build_python_packages.py` directly
   - Added AWS credentials configuration
   - Removed unused `MANYLINUX` env var
9. [ ] **Test end-to-end with CI-built wheels** - **PR #3261** (supersedes #3182)
   - Stacked on PR #3242 (setup_venv.py refactoring)
   - Builds Python packages for all artifact groups
   - Uploads to S3 and tests on GPU runners where available
   - Needs rebase once #3242 merges
10. [x] **File tracking issue** - **Issue #3177**
    - Covers ROCm Python, PyTorch, JAX, native Linux packages
    - Documents CI/CD divergence and unification plan

## Decisions Made

- **Bucket:** CI artifacts bucket (`therock-ci-artifacts`) for CI builds, NOT dev releases bucket
  - Path: `{run_id}-linux/python/{artifact_group}/` for per-run isolation
  - Keeps CI builds separate from actual dev/nightly/prerelease releases
- **Upload script:** New `upload_python_packages.py` (not extending `post_build_upload.py`)
  - Different input structure (packages dir vs build dir with logs/artifacts/manifests)
  - Uses same bucket selection logic (`retrieve_bucket_info()`)
  - Can migrate to `RunOutputRoot` when PR #3000 lands
- **Index strategy (target):** Deps-only base index + per-run rocm-only index (avoids version ambiguity)
- **Index strategy (shortcut):** Can skip index generation initially, use `--find-links` with downloaded wheels
- **Base index:** Architecture-neutral, reuse existing `update_dependencies.py` / `manage.py` tooling
- **Linux sanity check:** Skip local testing in build workflow, rely on GPU test workflow
- **Test workflow:** Separate `test_rocm_wheels.yml` (not inlined into build workflows)
  - Build workflows use CPU runners; testing needs GPU runners
  - Separate workflow enables testing wheels from any source (CI, nightly, dev)

### 2026-02-02 - PR #3182 Results and Follow-up

**PR #3182 results look okay overall.** However, one issue needs resolution before landing:

**Issue:** Linux::gfx1151 run of `rocm-sdk test` hit a 6-hour timeout.
- Logs: https://github.com/ROCm/TheRock/actions/runs/21533668625/job/62060067637?pr=3182

**Follow-up items:**

1. ~~**Add timeouts to `test_rocm_wheels.yml`:**~~ **Done - PR #3233 (merged)**
   - Job-level timeout: 30 minutes (catches hung downloads or any overall hang)
   - Step-level timeout on "Run rocm-sdk sanity tests" step: 5 minutes (catches hung tests)

2. **Investigate gfx1151 test failures:**
   - Determine why gfx1151 on Linux is not passing/hanging
   - Likely need to xfail/skip some tests to make progress

**PR #3214 merged:** Extracted container migration from PR #3182.
- Linux workflow now runs all steps in manylinux container (not just build step)
- Removes use of `linux_portable_build.py` from this workflow
- Prepares for S3 upload steps that need AWS CLI available

### 2026-02-03 - Added Timeouts to Test Workflow

**PR #3233 merged:** Add timeouts to `test_rocm_wheels.yml`.
- Job-level timeout: 30 minutes (catches hung downloads or overall hang)
- Step-level timeout on `rocm-sdk test` step: 5 minutes (catches hung tests quickly)
- Prevents 6-hour timeouts like seen with gfx1151 in PR #3182

### 2026-02-03 - setup_venv.py Refactoring

**Branch:** `setup-venv-find-links`
**PR:** https://github.com/ROCm/TheRock/pull/3242

Refactored `setup_venv.py` to improve testability and add new functionality (12 commits):

1. **`--find-links-url` support** - Install from flat pip indexes (S3 uploads)
2. **Explicit `--pre` and `--disable-cache` flags**
   - pip and uv use different syntax (`--pre` vs `--prerelease=allow`, `--no-cache-dir` vs `--no-cache`)
   - Script translates flags appropriately based on `--use-uv` setting
   - Initially tried `--extra-pip-args` but reverted due to pip/uv flag differences
3. **Whitespace delimiter for `--packages`** - Fixes conflict with pip extras syntax (`rocm[libraries,devel]`)
4. **Extracted `install_packages_into_venv()`** - Testable function with explicit parameters
5. **Simplified `_scrape_rocm_index_subdirs()`** - Returns union of all subdirs as `set[str] | None`
6. **Added assert for `find_venv_python_exe()`** - Fail fast if venv python not found
7. **Added unit tests** - 13 tests covering pip/uv paths, flags, multiple packages
   - Tests mock `find_venv_python_exe` and `run_command` to test command generation

**Manual testing commands:**

```bash
# Test with uv
python D:\projects\TheRock\build_tools/setup_venv.py test_uv.venv \
  --packages "rocm[libraries]==7.12.0.dev0" \
  --find-links=https://therock-artifacts-testing.s3.amazonaws.com/21440027240-windows/python/gfx110X-all/index.html \
  --clean --use-uv --pre

# Test with pip
python D:\projects\TheRock\build_tools/setup_venv.py test_pip.venv \
  --packages "rocm[libraries]==7.12.0.dev0" \
  --find-links=https://therock-artifacts-testing.s3.amazonaws.com/21440027240-windows/python/gfx110X-all/index.html \
  --clean --pre
```

### 2026-02-04 - PRs Merged and E2E Integration

Several PRs merged today:
- **PR #3214:** Container migration for Linux Python packages workflow
- **PR #3233:** Add timeouts to `test_rocm_wheels.yml`
- **PR #3136:** `upload_python_packages.py` script and documentation

**PR #3261 drafted:** Upload and test Python packages in CI workflows.
- Supersedes draft PR #3182
- Stacked on PR #3242 (`setup_venv.py` refactoring) - will need rebase once that merges
- Branch: `users/scotttodd/python-package-test-2`

Changes in PR #3261:
- Added `repository`/`ref` inputs to Linux and Windows Python package workflows
- Added AWS credentials configuration and `upload_python_packages.py` upload step
- Added `generate_target_to_run` and `test_rocm_wheels` jobs to both workflows
- Added `package_find_links_url` input to `test_rocm_wheels.yml`
- Renamed `package_index_url` → `package_index_url_base` for clarity
- Added `gha_set_output` call in `upload_python_packages.py` to set output URL
- Added `id-token: write` permission to `ci_linux.yml` and `ci_windows.yml`

Remaining in review: PR #3242 (`setup_venv.py` refactoring).

## Open Questions

- Should test workflow run on PRs or only after merge?
  - **Leaning:** At least on merge; optionally on PRs with GPU runner availability
- Which deps to include in base index? Same as nightly, or a subset?
- IAM permissions for CI artifacts bucket - does existing role work or need new one?
- Lifecycle policies for CI uploads - how long to retain per-run packages?

## Future Work

- **Pass `test_runs_on` through ci.yml:** Currently each Python packages workflow has its own
  `generate_target_to_run` job. This duplicates work already done in ci.yml's `setup` job.
  Consider adding `test_runs_on` as an optional input - when provided (from ci.yml), skip the
  generate job; when not provided (standalone/release triggers), compute it locally. This mirrors
  the `artifact_run_id` pattern.
